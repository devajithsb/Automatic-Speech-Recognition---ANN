{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsuXy8YfeogpSNWt0qsUAi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devajithsb/Automatic-Speech-Recognition---ANN/blob/main/ANN_working.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBrrgh5i6-4J",
        "outputId": "b80affc6-e9e2-439c-9c07-f2cfd1d46fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.11/dist-packages (3.14.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from SpeechRecognition) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoZUUSUw6lmW",
        "outputId": "7263f056-834a-4b9c-ef3f-5ccb8f97e97d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.1667 - loss: 39.6902 - val_accuracy: 0.0000e+00 - val_loss: 15.8314\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0000e+00 - loss: 36.9489 - val_accuracy: 0.0000e+00 - val_loss: 21.7970\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 18.8898 - val_accuracy: 0.0000e+00 - val_loss: 20.3339\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 18.1170 - val_accuracy: 0.0000e+00 - val_loss: 24.5048\n",
            "Epoch 5/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6667 - loss: 20.7059 - val_accuracy: 0.0000e+00 - val_loss: 26.4119\n",
            "Epoch 6/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.3333 - loss: 30.9746 - val_accuracy: 0.0000e+00 - val_loss: 25.7224\n",
            "Epoch 7/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.5000 - loss: 14.4438 - val_accuracy: 0.0000e+00 - val_loss: 24.8006\n",
            "Epoch 8/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.3333 - loss: 3.9837 - val_accuracy: 0.0000e+00 - val_loss: 20.9739\n",
            "Epoch 9/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.3333 - loss: 6.0768 - val_accuracy: 0.0000e+00 - val_loss: 15.8885\n",
            "Epoch 10/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.5000 - loss: 11.8546 - val_accuracy: 0.0000e+00 - val_loss: 12.2097\n",
            "Epoch 11/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1667 - loss: 18.3720 - val_accuracy: 0.0000e+00 - val_loss: 8.0336\n",
            "Epoch 12/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.3333 - loss: 6.2667 - val_accuracy: 0.0000e+00 - val_loss: 4.3266\n",
            "Epoch 13/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.3333 - loss: 9.7839 - val_accuracy: 0.5000 - val_loss: 2.1442\n",
            "Epoch 14/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 11.3470 - val_accuracy: 0.5000 - val_loss: 1.6561\n",
            "Epoch 15/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1667 - loss: 10.8505 - val_accuracy: 0.5000 - val_loss: 2.0275\n",
            "Epoch 16/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 9.1099 - val_accuracy: 0.5000 - val_loss: 2.8557\n",
            "Epoch 17/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5000 - loss: 5.9883 - val_accuracy: 0.0000e+00 - val_loss: 5.2620\n",
            "Epoch 18/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.6667 - loss: 4.7214 - val_accuracy: 0.0000e+00 - val_loss: 8.8925\n",
            "Epoch 19/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.5000 - loss: 7.7089 - val_accuracy: 0.0000e+00 - val_loss: 13.0719\n",
            "Epoch 20/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.3333 - loss: 17.5553 - val_accuracy: 0.0000e+00 - val_loss: 16.0555\n",
            "Could not understand audio: /content/sounds/Hello I m Rose From .wav\n",
            "\n",
            "Weighted Averages:\n",
            "Weighted Precision: 0.86\n",
            "Weighted Recall: 0.86\n",
            "Weighted F1-Score: 0.86\n",
            "Weighted Accuracy: 0.79\n"
          ]
        }
      ],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import speech_recognition as sr\n",
        "\n",
        "def extract_features(audio_file):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_file, sr=16000)\n",
        "        if len(y) == 0:\n",
        "            print(f\"Warning: {audio_file} is silent.\")\n",
        "            return np.zeros(13)\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "        if mfccs.shape[1] == 0:\n",
        "            print(f\"Warning: No MFCCs extracted from {audio_file}\")\n",
        "            return np.zeros(13)\n",
        "        return np.mean(mfccs.T, axis=0)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_file}: {e}\")\n",
        "        return np.zeros(13)\n",
        "\n",
        "def speech_to_text(audio_file):\n",
        "    try:\n",
        "        r = sr.Recognizer()\n",
        "        with sr.AudioFile(audio_file) as source:\n",
        "            audio_data = r.record(source)\n",
        "        return r.recognize_google(audio_data).lower()\n",
        "    except sr.UnknownValueError:\n",
        "        print(f\"Could not understand audio: {audio_file}\")\n",
        "        return None\n",
        "    except sr.RequestError as e:\n",
        "        print(f\"Could not request results: {e}\")\n",
        "        return None\n",
        "\n",
        "audio_files_and_labels = [\n",
        "    (\"/content/sounds/Life is a beautiful (alfred-british).wav\", \"Life is a beautiful journey\"),\n",
        "    (\"/content/sounds/Hello Myself Devajit.wav\", \"hello myself devajit\"),\n",
        "    (\"/content/sounds/Hello My self Ryan(canadian -ryan).wav\", \"hello myself ryan\"),\n",
        "    (\"/content/sounds/Hello My self Rishi(indian-rishi).wav\", \"hello myself rishi\"),\n",
        "    (\"/content/sounds/Life is a beautiful (shirley-scottish).wav\", \"Life is a beautiful journey\"),\n",
        "    (\"/content/sounds/Hello Myself Devajit (irish-cillian).wav\", \"hello myself Devajit\"),\n",
        "    (\"/content/sounds/Hello I m Rose From .wav\",\"Hello I'm Rose From New Zaeland\"),\n",
        "    (\"/content/sounds/Hello Myself Devajit (irish-cillian).wav\", \"hello myself Devajit\"),\n",
        "]\n",
        "X, y = [], []\n",
        "for audio_path, label in audio_files_and_labels:\n",
        "    if os.path.exists(audio_path):\n",
        "        features = extract_features(audio_path)\n",
        "        X.append(features)\n",
        "        y.append(label.lower())\n",
        "    else:\n",
        "        print(f\"Error: Audio file not found: {audio_path}\")\n",
        "\n",
        "if not X:\n",
        "    exit(\"Error: No valid features extracted.\")\n",
        "\n",
        "X = np.array(X)\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=8, validation_data=(X_test, y_test))\n",
        "\n",
        "def evaluate_model(ground_truth, predicted):\n",
        "    if not ground_truth or not predicted:\n",
        "        return {'precision': 0, 'recall': 0, 'f1_score': 0, 'accuracy': 0}\n",
        "\n",
        "    gt_words = set(ground_truth.lower().split())\n",
        "    pred_words = set(predicted.lower().split())\n",
        "\n",
        "    common = gt_words & pred_words\n",
        "    precision = len(common) / len(pred_words) if pred_words else 0\n",
        "    recall = len(common) / len(gt_words) if gt_words else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
        "    accuracy = len(common) / len(gt_words | pred_words) if (gt_words | pred_words) else 0\n",
        "\n",
        "    return {'precision': precision, 'recall': recall, 'f1_score': f1, 'accuracy': accuracy}\n",
        "\n",
        "total_precision = total_recall = total_f1 = total_accuracy = total_weight = 0\n",
        "\n",
        "for audio_path, ground_truth_label in audio_files_and_labels:\n",
        "    recognized_text = speech_to_text(audio_path)\n",
        "    if recognized_text:\n",
        "        metrics = evaluate_model(ground_truth_label, recognized_text)\n",
        "        total_precision += metrics['precision']\n",
        "        total_recall += metrics['recall']\n",
        "        total_f1 += metrics['f1_score']\n",
        "        total_accuracy += metrics['accuracy']\n",
        "        total_weight += 1\n",
        "\n",
        "if total_weight > 0:\n",
        "    print(\"\\nWeighted Averages:\")\n",
        "    print(f\"Weighted Precision: {total_precision / total_weight:.2f}\")\n",
        "    print(f\"Weighted Recall: {total_recall / total_weight:.2f}\")\n",
        "    print(f\"Weighted F1-Score: {total_f1 / total_weight:.2f}\")\n",
        "    print(f\"Weighted Accuracy: {total_accuracy / total_weight:.2f}\")\n",
        "else:\n",
        "    print(\"No valid predictions to calculate weighted averages.\")\n"
      ]
    }
  ]
}